{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./TRPO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from models import *\n",
    "from replay_memory import Memory\n",
    "from running_state import ZFilter\n",
    "from torch.autograd import Variable\n",
    "from trpo import trpo_step\n",
    "from utils import *\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./CityLearn/')\n",
    "from citylearn.my_citylearn import CityLearnEnv\n",
    "\n",
    "wandb_record = False\n",
    "if wandb_record:\n",
    "    import wandb\n",
    "    wandb.init(project=\"TRPO_rl\")\n",
    "    wandb.run.name = \"test_my_env\"\n",
    "wandb_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(gamma=0.995, env_name='HalfCheetah-v4', tau=0.97, l2_reg=0.001, max_kl=0.01, damping=0.1, seed=543, batch_size=15000, render=False, log_interval=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.995, metavar='G',\n",
    "                    help='discount factor (default: 0.995)')\n",
    "parser.add_argument('--env-name', default=\"HalfCheetah-v4\", metavar='G',\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--tau', type=float, default=0.97, metavar='G',\n",
    "                    help='gae (default: 0.97)')\n",
    "parser.add_argument('--l2-reg', type=float, default=1e-3, metavar='G',\n",
    "                    help='l2 regularization regression (default: 1e-3)')\n",
    "parser.add_argument('--max-kl', type=float, default=1e-2, metavar='G',\n",
    "                    help='max kl value (default: 1e-2)')\n",
    "parser.add_argument('--damping', type=float, default=1e-1, metavar='G',\n",
    "                    help='damping (default: 1e-1)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--batch-size', type=int, default=15000, metavar='N',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--render', action='store_true',\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args('')\n",
    "schema_filepath = './CityLearn/citylearn/data/my_data/schema.json'\n",
    "eval_schema_filepath = './CityLearn/citylearn/data/my_data/schema_eval.json'\n",
    "\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./CityLearn/citylearn/data/my_data/schema.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CityLearnEnv(schema_filepath)\n",
    "# env = gym.make(args.env_name)\n",
    "\n",
    "num_inputs = env.observation_space[0].shape[0]\n",
    "num_actions = env.action_space[0].shape[0]\n",
    "\n",
    "num_inputs, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "states = []\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(actions=[0.1,0.2,0.3,0.1,.1])\n",
    "    states.append(state)\n",
    "\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "policy_net = Policy(num_inputs, num_actions)\n",
    "value_net = Value(num_inputs)\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).unsqueeze(0)\n",
    "    action_mean, _, action_std = policy_net(Variable(state))\n",
    "    action = torch.normal(action_mean, action_std)\n",
    "    return action\n",
    "\n",
    "def select_action_eval(state):\n",
    "    state = torch.from_numpy(state).unsqueeze(0)\n",
    "    action_mean, _, _ = policy_net(Variable(state))\n",
    "    return action_mean\n",
    "\n",
    "def update_params(batch):\n",
    "    rewards = torch.Tensor(batch.reward)\n",
    "    masks = torch.Tensor(batch.mask)\n",
    "    actions = torch.Tensor(np.concatenate(batch.action, 0))\n",
    "    states = torch.Tensor(batch.state)\n",
    "    values = value_net(Variable(states))\n",
    "\n",
    "    returns = torch.Tensor(actions.size(0),1)\n",
    "    deltas = torch.Tensor(actions.size(0),1)\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_return = 0\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n",
    "        deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values.data[i]\n",
    "        advantages[i] = deltas[i] + args.gamma * args.tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_return = returns[i, 0]\n",
    "        prev_value = values.data[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    targets = Variable(returns)\n",
    "\n",
    "    # Original code uses the same LBFGS to optimize the value loss\n",
    "    def get_value_loss(flat_params):\n",
    "        set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
    "        for param in value_net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.fill_(0)\n",
    "\n",
    "        values_ = value_net(Variable(states))\n",
    "\n",
    "        value_loss = (values_ - targets).pow(2).mean()\n",
    "\n",
    "        # weight decay\n",
    "        for param in value_net.parameters():\n",
    "            value_loss += param.pow(2).sum() * args.l2_reg\n",
    "        value_loss.backward()\n",
    "        return (value_loss.data.double().numpy(), get_flat_grad_from(value_net).data.double().numpy())\n",
    "\n",
    "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(value_net).double().numpy(), maxiter=25)\n",
    "    set_flat_params_to(value_net, torch.Tensor(flat_params))\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "    fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n",
    "\n",
    "    def get_loss(volatile=False):\n",
    "        if volatile:\n",
    "            with torch.no_grad():\n",
    "                action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "        else:\n",
    "            action_means, action_log_stds, action_stds = policy_net(Variable(states))\n",
    "                \n",
    "        log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n",
    "        action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n",
    "        return action_loss.mean()\n",
    "\n",
    "\n",
    "    def get_kl():\n",
    "        mean1, log_std1, std1 = policy_net(Variable(states))\n",
    "\n",
    "        mean0 = Variable(mean1.data)\n",
    "        log_std0 = Variable(log_std1.data)\n",
    "        std0 = Variable(std1.data)\n",
    "        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    trpo_step(policy_net, get_loss, get_kl, args.max_kl, args.damping)\n",
    "\n",
    "running_state = ZFilter((num_inputs,), clip=5)\n",
    "running_reward = ZFilter((1,), demean=False, clip=10)\n",
    "\n",
    "def evaluation(schema_dict_eval):\n",
    "    eval_env = CityLearnEnv(schema_dict_eval)\n",
    "    eval_reward = 0.\n",
    "\n",
    "    done = False\n",
    "    state = eval_env.reset()\n",
    "    # state = [running_state[i](state[i][:-building_count]) for i in range(building_count)]\n",
    "    state = running_state(state[0])\n",
    "\n",
    "    while not done:\n",
    "        action = select_action_eval(state).data[0].numpy()\n",
    "        print(\"{:.2f}\".format(action.item()), end=\", \")\n",
    "        next_state, reward, done, _ = eval_env.step(action)\n",
    "        eval_reward += reward[0]\n",
    "\n",
    "        # state = [running_state[i](next_state[i][:-building_count]) for i in range(building_count)]\n",
    "        state = running_state(next_state[0])\n",
    "    print()\n",
    "\n",
    "    print('evaluate reward {:.2f}'.format(eval_reward))\n",
    "\n",
    "    if wandb_record:\n",
    "        wandb.log({\"eval_\": eval_reward}, step = int(wandb_step))\n",
    "\n",
    "\n",
    "with open(schema_filepath) as json_file:\n",
    "    schema_dict = json.load(json_file)\n",
    "with open(eval_schema_filepath) as json_eval_file:\n",
    "    schema_dict_eval = json.load(json_eval_file)\n",
    "\n",
    "schema_dict[\"personalization\"] = False\n",
    "schema_dict_eval[\"personalization\"] = False\n",
    "\n",
    "for b_i in range(5):\n",
    "    if b_i == 0:\n",
    "        continue\n",
    "    schema_dict[\"buildings\"][\"Building_\"+str(b_i+1)][\"include\"] = False\n",
    "    schema_dict_eval[\"buildings\"][\"Building_\"+str(b_i+1)][\"include\"] = False\n",
    "\n",
    "env = CityLearnEnv(schema_dict)\n",
    "\n",
    "for i_episode in count(1):\n",
    "    memory = Memory()\n",
    "\n",
    "    num_steps = 0\n",
    "    reward_batch = 0\n",
    "    num_episodes = 0\n",
    "    while num_steps < args.batch_size:\n",
    "        state = env.reset()[0]\n",
    "        state = running_state(state)\n",
    "\n",
    "        reward_sum = 0\n",
    "        for t in range(10000): # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            action = action.data[0].numpy()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward[0]\n",
    "\n",
    "            next_state = running_state(next_state[0])\n",
    "\n",
    "            mask = 1\n",
    "            if done:\n",
    "                mask = 0\n",
    "\n",
    "            memory.push(state, np.array([action]), mask, next_state, reward[0])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "        num_steps += (t-1)\n",
    "        num_episodes += 1\n",
    "        reward_batch += reward_sum\n",
    "\n",
    "    reward_batch /= num_episodes\n",
    "    batch = memory.sample_batch()\n",
    "    update_params(batch)\n",
    "    wandb_step += 1\n",
    "\n",
    "    if i_episode % args.log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {}\\tAverage reward {:.2f}'.format(\n",
    "            i_episode, reward_sum, reward_batch))\n",
    "        if wandb_record:\n",
    "            wandb.log({\"train\": reward_sum}, step = int(wandb_step))\n",
    "    \n",
    "    evaluation(schema_dict_eval)\n",
    "        \n",
    "    if i_episode > 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
