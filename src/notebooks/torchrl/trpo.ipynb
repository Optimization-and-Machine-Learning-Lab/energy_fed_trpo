{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from src.utils.cl_rewards import *\n",
    "from src.utils.utils import get_env_from_config, set_seed\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import Composite, Categorical, Bounded, Unbounded\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv, EnvBase\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators, ReinforceLoss, A2CLoss, KLPENPPOLoss\n",
    "\n",
    "# Utils\n",
    "\n",
    "from torch import multiprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import NormalizeReward\n",
    "from citylearn.wrappers import (\n",
    "    NormalizedObservationWrapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS = {\n",
    "    'cost': Cost,\n",
    "    'weighted_cost_emissions': WeightedCostAndEmissions,\n",
    "    'cost_pen_no_batt': CostNoBattPenalization,\n",
    "    'cost_pen_bad_batt': CostBadBattUsePenalization,\n",
    "    'cost_pen_bad_action': CostIneffectiveActionPenalization,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards_and_actions(train_rewards, eval_rewards, train_env, eval_env, policy):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "    # Plot rewards\n",
    "    axs[0, 0].plot(train_rewards, label=\"Training Reward Mean\")\n",
    "    axs[0, 0].plot(eval_rewards, label=\"Evaluation Reward Mean\")\n",
    "    axs[0, 0].set_xlabel(\"Training Iterations\")\n",
    "    axs[0, 0].set_ylabel(\"Reward\")\n",
    "    axs[0, 0].set_title(\"Training and Evaluation Rewards\")\n",
    "    axs[0, 0].grid()\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Sample actions for train_env and eval_env\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        train_rollout = train_env.rollout(train_env.cl_env.unwrapped.time_steps - 1, policy=policy)\n",
    "        train_actions = train_rollout.get(train_env.action_key).squeeze()\n",
    "        train_soc = [b.electrical_storage.soc for b in train_env.cl_env.unwrapped.buildings]\n",
    "        train_net_electricity_consumption = [b.net_electricity_consumption for b in train_env.cl_env.unwrapped.buildings]\n",
    "        train_opt_actions = torch.tensor(np.array(train_env.cl_env.unwrapped.optimal_actions), requires_grad=False).swapaxes(0, 1)\n",
    "        train_opt_soc = torch.tensor(np.array(train_env.cl_env.unwrapped.optimal_soc), requires_grad=False).swapaxes(0, 1)\n",
    "\n",
    "        eval_rollout = eval_env.rollout(eval_env.cl_env.unwrapped.time_steps - 1, policy=policy)\n",
    "        eval_actions = eval_rollout.get(eval_env.action_key).squeeze()\n",
    "        eval_soc = [b.electrical_storage.soc for b in eval_env.cl_env.unwrapped.buildings]\n",
    "        eval_net_electricity_consumption = [b.net_electricity_consumption for b in eval_env.cl_env.unwrapped.buildings]\n",
    "        eval_opt_actions = torch.tensor(np.array(eval_env.cl_env.unwrapped.optimal_actions), requires_grad=False).swapaxes(0, 1)\n",
    "        eval_opt_soc = torch.tensor(np.array(eval_env.cl_env.unwrapped.optimal_soc), requires_grad=False).swapaxes(0, 1)\n",
    "\n",
    "    # Plot actions for each building\n",
    "    for i in range(train_env.n_agents):\n",
    "        axs[0, 1].plot(train_actions[:, i].numpy(), label=f\"Train Agent {i} actions\")\n",
    "        axs[0, 1].plot(train_opt_actions[:, i], label=f\"Train Agent {i} optimal actions\", linestyle=\"--\")\n",
    "        axs[0, 2].plot(eval_actions[:, i].numpy(), label=f\"Eval Agent {i} actions\")\n",
    "        axs[0, 2].plot(eval_opt_actions[:, i], label=f\"Eval Agent {i} optimal actions\", linestyle=\"--\")\n",
    "\n",
    "    axs[0, 1].set_xlabel(\"Hour of the day\")\n",
    "    axs[0, 1].set_ylabel(\"Action\")\n",
    "    axs[0, 1].set_title(\"Train Actions Comparison\")\n",
    "    axs[0, 1].grid()\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[0, 2].set_xlabel(\"Hour of the day\")\n",
    "    axs[0, 2].set_ylabel(\"Action\")\n",
    "    axs[0, 2].set_title(\"Eval Actions Comparison\")\n",
    "    axs[0, 2].grid()\n",
    "    axs[0, 2].legend()\n",
    "\n",
    "    # Plot SOC for each building\n",
    "    for i in range(train_env.n_agents):\n",
    "        axs[1, 1].plot(train_soc[i], label=f\"Train Agent {i} SOC\")\n",
    "        axs[1, 1].plot(train_opt_soc[:, i], label=f\"Train Agent {i} optimal SOC\", linestyle=\"--\")\n",
    "        axs[1, 2].plot(eval_soc[i], label=f\"Eval Agent {i} SOC\")\n",
    "        axs[1, 2].plot(eval_opt_soc[:, i], label=f\"Eval Agent {i} optimal SOC\", linestyle=\"--\")\n",
    "\n",
    "    axs[1, 1].set_xlabel(\"Hour of the day\")\n",
    "    axs[1, 1].set_ylabel(\"SOC\")\n",
    "    axs[1, 1].set_title(\"Train SOC Comparison\")\n",
    "    axs[1, 1].grid()\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[1, 2].set_xlabel(\"Hour of the day\")\n",
    "    axs[1, 2].set_ylabel(\"SOC\")\n",
    "    axs[1, 2].set_title(\"Eval SOC Comparison\")\n",
    "    axs[1, 2].grid()\n",
    "    axs[1, 2].legend()\n",
    "\n",
    "    # Plot net electricity consumption for each building\n",
    "    for i in range(train_env.n_agents):\n",
    "        axs[1, 0].plot(train_net_electricity_consumption[i], label=f\"Train Agent {i} Net Electricity Consumption\")\n",
    "        axs[1, 0].plot(eval_net_electricity_consumption[i], label=f\"Eval Agent {i} Net Electricity Consumption\", linestyle=\"--\")\n",
    "\n",
    "    axs[1, 0].set_xlabel(\"Hour of the day\")\n",
    "    axs[1, 0].set_ylabel(\"Net Electricity Consumption\")\n",
    "    axs[1, 0].set_title(\"Net Electricity Consumption Comparison\")\n",
    "    axs[1, 0].grid()\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    plt.suptitle(f'Results for day {train_env.cl_env.unwrapped.episode_tracker.episode_start_time_step}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CityLearn wrapper for TorchRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityLearnMultiAgentEnv(EnvBase):\n",
    "    \n",
    "    def __init__(self, env_config, device, seed, batch_size=1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cl_env = get_env_from_config(config=env_config, seed=seed)\n",
    "        self.cl_env = NormalizeReward(self.cl_env, gamma=1, epsilon=1e-8)\n",
    "        self.cl_env = NormalizedObservationWrapper(self.cl_env)\n",
    "        self.n_agents = len(env_config[\"schema\"]['buildings'])\n",
    "        self.batch_size = torch.Size([batch_size,])\n",
    "        self.device = device\n",
    "\n",
    "        action_specs = []\n",
    "        observation_specs = []\n",
    "        reward_specs = []\n",
    "        \n",
    "        cl_env_as = self.cl_env.action_space\n",
    "        cl_env_os = self.cl_env.observation_space\n",
    "        self.n_actions = cl_env_as[0].shape[0]\n",
    "        self.n_observations = cl_env_os[0].shape[0]\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            \n",
    "            action_specs.append(Bounded(\n",
    "                low=cl_env_as[i].low,\n",
    "                high=cl_env_as[i].high,\n",
    "                shape=cl_env_as[i].shape,\n",
    "                device=device\n",
    "            ))\n",
    "            reward_specs.append(Unbounded\n",
    "                (shape=(1,), dtype=torch.float, device=device),\n",
    "            )\n",
    "            observation_specs.append(Bounded(\n",
    "                low=cl_env_os[i].low,\n",
    "                high=cl_env_os[i].high,\n",
    "                shape=cl_env_os[i].shape,\n",
    "                device=device\n",
    "            ))\n",
    "\n",
    "        # Define observation and action spaces\n",
    "\n",
    "        self.action_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"action\": torch.stack(action_specs, dim=0).view(self.batch_size[0], self.n_agents, cl_env_as[0].shape[0])},\n",
    "                shape=(self.batch_size[0], self.n_agents,)\n",
    "            )\n",
    "        }, batch_size=self.batch_size)\n",
    "\n",
    "        self.unbatched_action_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"action\": torch.stack(action_specs, dim=0).view(self.n_agents, cl_env_as[0].shape[0])},\n",
    "                shape=(self.n_agents,)\n",
    "            )\n",
    "        })\n",
    "\n",
    "        self.reward_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"reward\": torch.stack(reward_specs, dim=0).view(self.batch_size[0], self.n_agents, 1)},\n",
    "                shape=(self.batch_size[0], self.n_agents,)\n",
    "            )\n",
    "        }, batch_size=self.batch_size)\n",
    "\n",
    "        self.unbatched_reward_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"reward\": torch.stack(reward_specs, dim=0).view(self.n_agents, 1)},\n",
    "                shape=(self.n_agents,)\n",
    "            )\n",
    "        })\n",
    "\n",
    "        self.observation_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"observation\": torch.stack(observation_specs, dim=0).expand(self.batch_size[0], self.n_agents, cl_env_os[0].shape[0])},\n",
    "                shape=(self.batch_size[0], self.n_agents,)\n",
    "            )\n",
    "        }, batch_size=self.batch_size)\n",
    "\n",
    "        self.unbatched_observation_spec = Composite({\n",
    "            \"agents\": Composite(\n",
    "                {\"observation\": torch.stack(observation_specs, dim=0).expand(self.n_agents, cl_env_os[0].shape[0])},\n",
    "                shape=(self.n_agents,)\n",
    "            )\n",
    "        })\n",
    "\n",
    "        self.done_spec = Categorical(n=2, shape=torch.Size((self.batch_size[0], )), dtype=torch.bool)\n",
    "\n",
    "        # Initialize state variables\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "\n",
    "    def _reset(self, tensordict=None):\n",
    "        observations, _ = self.cl_env.reset()\n",
    "        tensordict = TensorDict({\n",
    "            \"agents\": TensorDict({\n",
    "                # \"info\": torch.empty(self.batch_size),\n",
    "                \"observation\": torch.tensor(\n",
    "                    np.array(observations), dtype=torch.float\n",
    "                ).reshape(self.batch_size[0], len(observations), len(observations[0])),\n",
    "            }, torch.Size((self.batch_size[0], self.n_agents)), device=self.device),\n",
    "            \"done\": torch.tensor(False, dtype=torch.bool).repeat(*self.batch_size)\n",
    "        }, batch_size=self.batch_size, device=self.device)\n",
    "        \n",
    "        return tensordict\n",
    "\n",
    "    def _step(self, tensordict):\n",
    "       \n",
    "        # Step through the environment\n",
    "        next_obs, rewards, done, _, _ = self.cl_env.step(tensordict['agents','action'].cpu().squeeze(0).numpy())\n",
    "        self.done = done\n",
    "\n",
    "        # Prepare TensorDict for the step\n",
    "        step_results = TensorDict({\n",
    "            \"agents\": TensorDict({\n",
    "                \"reward\": torch.tensor(\n",
    "                    rewards, dtype=torch.float\n",
    "                ).reshape(self.batch_size[0], self.n_agents, self.n_actions),\n",
    "                \"observation\": torch.tensor(\n",
    "                    np.array(next_obs), dtype=torch.float\n",
    "                ).reshape(self.batch_size[0], self.n_agents, self.n_observations),\n",
    "                # \"info\": torch.empty(self.batch_size),\n",
    "            }, batch_size=torch.Size((self.batch_size[0], self.n_agents)), device=self.device),\n",
    "            \"done\": torch.tensor(done, dtype=torch.bool).repeat(*self.batch_size)\n",
    "        }, batch_size=self.batch_size, device=self.device)\n",
    "\n",
    "        return step_results\n",
    "\n",
    "    def _set_seed(self, seed):\n",
    "        self.cl_env.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def close(self):\n",
    "        self.cl_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility to create a configured environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_config, device, seed):\n",
    "    \n",
    "    env = CityLearnMultiAgentEnv(\n",
    "        env_config=env_config,\n",
    "        device=device,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Include reward sum in the environment\n",
    "    env = TransformedEnv(\n",
    "        env,\n",
    "        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    "    )\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common configurations for environments\n",
    "\n",
    "active_observations = [\n",
    "    'hour',\n",
    "    'day_type',\n",
    "    'solar_generation',\n",
    "    'net_electricity_consumption',\n",
    "    'electrical_storage_soc',\n",
    "    'non_shiftable_load',\n",
    "    'direct_solar_irradiance',\n",
    "    'direct_solar_irradiance_predicted_6h',\n",
    "    'direct_solar_irradiance_predicted_12h',\n",
    "    'direct_solar_irradiance_predicted_24h',\n",
    "    'selling_price'\n",
    "]\n",
    "\n",
    "data_path = 'data/naive_data/'\n",
    "reward = 'weighted_cost_emissions'\n",
    "seed = 0\n",
    "price_margin = 0.1\n",
    "day_count = 1\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "device_ix = 7\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(device_ix)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "# Training configurations\n",
    "\n",
    "schema_filepath = data_path + 'schema.json'\n",
    "\n",
    "with open(schema_filepath) as json_file:\n",
    "    schema_dict = json.load(json_file)\n",
    "\n",
    "train_env_config = {\n",
    "    \"schema\": schema_dict,\n",
    "    \"central_agent\": False,\n",
    "    \"active_observations\": active_observations,\n",
    "    \"reward_function\": REWARDS[reward],\n",
    "    \"random_seed\": seed,\n",
    "    \"day_count\": day_count,\n",
    "    \"extended_obs\": True,\n",
    "    \"price_margin\": price_margin,\n",
    "    \"personal_encoding\": True,\n",
    "}\n",
    "\n",
    "train_env = create_env(train_env_config, device, seed)\n",
    "\n",
    "# Validation configurations\n",
    "\n",
    "schema_filepath = data_path + 'eval/schema.json'\n",
    "\n",
    "with open(schema_filepath) as json_file:\n",
    "    schema_dict = json.load(json_file)\n",
    "\n",
    "eval_env_config = {\n",
    "    \"schema\": schema_dict,\n",
    "    \"central_agent\": False,\n",
    "    \"active_observations\": active_observations,\n",
    "    \"reward_function\": REWARDS[reward],\n",
    "    \"random_seed\": seed,\n",
    "    \"day_count\": day_count,\n",
    "    \"extended_obs\": True,\n",
    "    \"price_margin\": price_margin,\n",
    "    \"personal_encoding\": True,\n",
    "}\n",
    "\n",
    "eval_env = create_env(eval_env_config, device, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check specs for training and validation environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"action_spec:\", train_env.full_action_spec)\n",
    "print(\"reward_spec:\", train_env.full_reward_spec)\n",
    "print(\"done_spec:\", train_env.full_done_spec)\n",
    "print(\"observation_spec:\", train_env.full_observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"action_spec:\", eval_env.full_action_spec)\n",
    "print(\"reward_spec:\", eval_env.full_reward_spec)\n",
    "print(\"done_spec:\", eval_env.full_done_spec)\n",
    "print(\"observation_spec:\", eval_env.full_observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(train_env)\n",
    "check_env_specs(eval_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probabilistic_policy(env, share_parameters_policy=True, device=\"cpu\"):\n",
    "\n",
    "    # First: define a neural network n_obs_per_agent -> 2 * n_actions_per_agents (mean and std)\n",
    "\n",
    "    policy_net = torch.nn.Sequential(\n",
    "        MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=2 * env.action_spec.shape[-1],\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=False,\n",
    "            share_params=share_parameters_policy,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        NormalParamExtractor(),\n",
    "    )\n",
    "\n",
    "    # Second: wrap the neural network in a TensordictModule\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    )\n",
    "\n",
    "    # Third: define the probabilistic policy\n",
    "\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=env.unbatched_action_spec,\n",
    "        in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "        out_keys=[env.action_key],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "            \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "    )  # we'll need the log-prob for the PPO loss\n",
    "\n",
    "    return policy\n",
    "\n",
    "policy = create_probabilistic_policy(train_env, share_parameters_policy=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(env, share_parameters_critic=True, mappo=True, device=\"cpu\"):\n",
    "\n",
    "    critic_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=1,  # 1 value per agent\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=mappo,\n",
    "        share_params=share_parameters_critic,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    critic = TensorDictModule(\n",
    "        module=critic_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"state_value\")],\n",
    "    )\n",
    "    \n",
    "    return critic\n",
    "\n",
    "critic = create_critic(train_env, share_parameters_critic=True, mappo=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the Actor and Critic are well configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running policy:\", policy(train_env.reset()))\n",
    "print(\"Running value:\", critic(train_env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(env, policy, device, frames_per_batch, n_iters):\n",
    "    total_frames = frames_per_batch * n_iters\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        policy,\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )\n",
    "    return collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(frames_per_batch, device, minibatch_size):\n",
    "    return ReplayBuffer(\n",
    "        storage=LazyTensorStorage(\n",
    "            frames_per_batch, device=device\n",
    "        ),  # We store the frames_per_batch collected at each iteration\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_module(policy, critic, env, clip_epsilon, entropy_eps, gamma, lmbda):\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor_network=policy,\n",
    "        critic_network=critic,\n",
    "        clip_epsilon=clip_epsilon,\n",
    "        entropy_coef=entropy_eps,\n",
    "        normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    "    )\n",
    "    loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "        reward=env.reward_key,\n",
    "        action=env.action_key,\n",
    "        sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "        value=(\"agents\", \"state_value\"),\n",
    "        # These last 2 keys will be expanded to match the reward shape\n",
    "        done=(\"agents\", \"done\"),\n",
    "        terminated=(\"agents\", \"terminated\"),\n",
    "    )\n",
    "\n",
    "    loss_module.make_value_estimator(\n",
    "        ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    "    )  # We build GAE\n",
    "    \n",
    "    return loss_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(\n",
    "        env, eval_env, n_iters, collector, loss_module, replay_buffer, num_epochs, frames_per_batch, minibatch_size, max_grad_norm, optim\n",
    "    ):\n",
    "\n",
    "    episode_reward_mean_list = []\n",
    "    episode_reward_mean_list_eval = []\n",
    "\n",
    "    GAE = loss_module.value_estimator\n",
    "\n",
    "    with tqdm(total=n_iters, nrows=10, desc=\"episode: 0, reward_mean: 0, eval_reward_mean: 0\") as pbar:\n",
    "\n",
    "        episode = 0\n",
    "\n",
    "        for tensordict_data in collector:\n",
    "\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"done\"),\n",
    "                tensordict_data.get((\"next\", \"done\"))\n",
    "                .unsqueeze(-1)\n",
    "                .repeat(1, 1, env.n_agents)\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "            )\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"terminated\"),\n",
    "                tensordict_data.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .repeat(1, 1, env.n_agents)\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "            )\n",
    "\n",
    "            # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                GAE(\n",
    "                    tensordict_data,\n",
    "                    params=loss_module.critic_network_params,\n",
    "                    target_params=loss_module.target_critic_network_params,\n",
    "                )  # Compute GAE and add it to the data\n",
    "\n",
    "            data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "            replay_buffer.extend(data_view)\n",
    "\n",
    "            for _ in range(num_epochs):\n",
    "\n",
    "                for _ in range(frames_per_batch // minibatch_size):\n",
    "\n",
    "                    subdata = replay_buffer.sample()\n",
    "                    loss_vals = loss_module(subdata)\n",
    "\n",
    "                    loss_value = (\n",
    "                        loss_vals[\"loss_objective\"]\n",
    "                        + loss_vals[\"loss_critic\"]\n",
    "                        + loss_vals[\"loss_entropy\"]\n",
    "                    )\n",
    "\n",
    "                    loss_value.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        loss_module.parameters(), max_grad_norm\n",
    "                    )  # Optional\n",
    "\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "\n",
    "            # Evaluating\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                policy.eval()\n",
    "\n",
    "                episode_reward_mean_eval = 0\n",
    "\n",
    "                for _ in range(minibatch_size):\n",
    "\n",
    "                    rollout = eval_env.rollout((eval_env.cl_env.unwrapped.time_steps - 1), policy=policy)\n",
    "                    episode_reward_mean_eval += rollout.get((\"next\", \"agents\", \"episode_reward\")).mean().item()\n",
    "\n",
    "                episode_reward_mean_eval = episode_reward_mean_eval / minibatch_size\n",
    "\n",
    "                episode_reward_mean_list_eval.append(episode_reward_mean_eval)\n",
    "                policy.train()\n",
    "\n",
    "            # Logging\n",
    "\n",
    "            done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "\n",
    "            episode_reward_mean = (\n",
    "                tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "            )\n",
    "\n",
    "            episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            pbar.set_description(f\"episode: {episode}, reward_mean: {episode_reward_mean}, eval_reward_mean: {episode_reward_mean_eval}\")\n",
    "            pbar.update()\n",
    "\n",
    "    return policy, episode_reward_mean_list, episode_reward_mean_list_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "days_per_batch = 512\n",
    "frames_per_batch = days_per_batch * 24  # Number of team frames collected per training iteration\n",
    "n_iters = 25  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 256  # Size of the mini-batches in each optimization step\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "lr = 1e-3  # Learning rate\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 1  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-3  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "# Create networks\n",
    "policy = create_probabilistic_policy(train_env, share_parameters_policy=False, device=device)\n",
    "critic = create_critic(train_env, share_parameters_critic=True, mappo=True, device=device)\n",
    "\n",
    "# Configure data collection\n",
    "collector = sample_data(train_env, policy, device, frames_per_batch, n_iters)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = create_replay_buffer(frames_per_batch, device, minibatch_size)\n",
    "\n",
    "# Create loss module\n",
    "loss_module = create_loss_module(policy, critic, train_env, clip_epsilon, entropy_eps, gamma, lmbda)\n",
    "\n",
    "# Create optimizer\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "\n",
    "# Train policy\n",
    "\n",
    "ppo_policy, ppo_reward_mean_list, ppo_eval_reward_mean_list = train_policy(\n",
    "    env=train_env,\n",
    "    eval_env=eval_env,\n",
    "    n_iters=n_iters,\n",
    "    collector=collector,\n",
    "    loss_module=loss_module,\n",
    "    replay_buffer=replay_buffer,\n",
    "    num_epochs=num_epochs,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    minibatch_size=minibatch_size,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    optim=optim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_and_actions(ppo_reward_mean_list, ppo_eval_reward_mean_list, train_env, eval_env, ppo_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_rollout = eval_env.rollout(eval_env.cl_env.unwrapped.time_steps - 1, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.cl_env.unwrapped.buildings[0].electrical_storage.soc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.cl_env.unwrapped.optimal_soc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.cl_env.unwrapped.episode_tracker.episode_start_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
